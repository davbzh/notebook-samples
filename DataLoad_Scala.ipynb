{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into a notebook from different sources (Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start analyzing your data, you have to load the data from a data source. You can store your data in many different data sources. This reference notebook shows you how to load and integrate data in a notebook from the following data sources:\n",
    "-  Object Storage V3\n",
    "-  dashDB\n",
    "-  Cloudant\n",
    "-  PostgreSQL\n",
    "\n",
    "The notebook sample code shows you how to load data into a notebook by using Scala. You can copy and paste these code snippets into the notebook you are developing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Load data from Object Storage V3](#osv3)\n",
    "  - [Load data by using Scala](#osv3_scala)\n",
    "  - [Load data by using Stocator](#osv3_stocator)\n",
    "- [Load data from dashDB](#dashdb)\n",
    "- [Load data from a Cloudant database](#cloudant)\n",
    "- [Load data from a PostgreSQL database](#postgresql)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3\"></a>\n",
    "## Load data from Object Storage V3\n",
    "IBM® Object Storage for Bluemix® provides you with access to a fully provisioned Swift Object Storage account to manage your data. Object Storage uses OpenStack Identity (Keystone) for authentication and can be accessed directly by using [OpenStack Object Storage (Swift) API v3](http://developer.openstack.org/api-ref-identity-v3.html#credentials-v3). \n",
    "\n",
    "When you load data for use in a notebook, the data file is stored in the Object Storage instance associated with your Spark service.\n",
    "\n",
    "Click the next code cell to set the focus on the cell. Now add the credentials to access the data file to this code cell by selecting **Palette>Data Sources** and clicking the `Insert to code` function below the data file in the **Data Source** pane.\n",
    "\n",
    "When you select the `Insert to code` function, a code cell with a Scala hashmap is created for you. Adjust the credentials in the dictionary to correspond with the credentials inserted by the `Insert to code function` and run the dictionary code cell. The access credentials to the Object Storage instance in the dictionary are provided for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3_scala\"></a>\n",
    "### Load data by using Scala\n",
    "\n",
    "Run the next cells to load the data from a file in Object Storage by using Scala functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import scala.collection.breakOut\n",
    "\n",
    "def setConfig(name:String, dsConfiguration:String) : Unit = {\n",
    "    val pfx = \"fs.swift.service.\" + name\n",
    "    val settings:Map[String,String] = dsConfiguration.split(\"\\\\n\").\n",
    "        map(l=>(l.split(\":\",2)(0).trim(), l.split(\":\",2)(1).trim()))(breakOut)\n",
    "\n",
    "    val conf = sc.getConf\n",
    "    conf.set(pfx + \"auth.url\", settings.getOrElse(\"auth_url\",\"\"))\n",
    "    conf.set(pfx + \"tenant\", settings.getOrElse(\"tenantId\", \"\"))\n",
    "    conf.set(pfx + \"username\", settings.getOrElse(\"username\", \"\"))\n",
    "    conf.set(pfx + \"password\", settings.getOrElse(\"password\", \"\"))\n",
    "    conf.set(pfx + \"apikey\", settings.getOrElse(\"password\", \"\"))\n",
    "    conf.set(pfx + \"auth.endpoint.prefix\", \"endpoints\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setConfig(\"spark\", credentials.toString())\n",
    "\n",
    "val sqlctx = new SQLContext(sc)\n",
    "val scplain = sqlctx.sparkContext\n",
    "sqlctx.setConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "import sqlctx.implicits._\n",
    "\n",
    "val df = (sqlctx.read\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferschema\",\"true\")\n",
    "    .option(\"mode\",\"DROPMALFORMED\")\n",
    "    .load(\"swift://notebooks.spark/\" + credentials(\"filename\"))\n",
    ")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a Spark DataFrame and you can begin analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"osv3_stocator\"></a>\n",
    "### Load data using Stocator\n",
    "Stocator is a storage connector for Spark that eliminates some of the unnecessary Hadoop drivers that are not needed to interact with object storage. Stocator's Hadoop configuration can be set by using the following configuration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import scala.util.control.NonFatal\n",
    "import play.api.libs.json.Json\n",
    "\n",
    "val sqlctx = new SQLContext(sc)\n",
    "val scplain = sqlctx.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can access data in the data file in Object Storage by using the [`SparkContext`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.html#pyspark.SparkContext) object, you must set the Hadoop configuration by using the following configuration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setRemoteObjectStorageConfig(name:String, sc: SparkContext, dsConfiguration:String) : Boolean = {\n",
    "    try {\n",
    "        val result = scala.util.parsing.json.JSON.parseFull(dsConfiguration)\n",
    "        result match {\n",
    "            case Some(e:Map[String,String]) => {\n",
    "                val prefix = \"fs.swift2d.service.\" + name\n",
    "                val hconf = sc.hadoopConfiguration\n",
    "                hconf.set(\"fs.swift2d.impl\",\"com.ibm.stocator.fs.ObjectStoreFileSystem\")\n",
    "                hconf.set(prefix + \".auth.url\", e(\"auth_url\") + \"/v3/auth/tokens\")\n",
    "                hconf.set(prefix + \".tenant\", e(\"project_id\"))\n",
    "                hconf.set(prefix + \".username\", e(\"user_id\"))\n",
    "                hconf.set(prefix + \".password\", e(\"password\"))\n",
    "                hconf.set(prefix + \"auth.method\", \"keystoneV3\")\n",
    "                hconf.set(prefix + \".region\", e(\"region\"))\n",
    "                hconf.setBoolean(prefix + \".public\", true)\n",
    "                println(\"Successfully modified sparkcontext object with remote Object Storage Credentials using datasource name \" + name)\n",
    "                println(\"\")\n",
    "                return true\n",
    "            }\n",
    "            case None => println(\"Failed.\")\n",
    "                return false\n",
    "        }\n",
    "    }\n",
    "    catch {\n",
    "       case NonFatal(exc) => println(exc)\n",
    "           return false\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Hadoop configuration and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val setObjStor = setRemoteObjectStorageConfig(\"sparksql\", scplain, Json.toJson(credentials.toMap).toString)\n",
    "val data_rdd = sc.textFile(\"swift2d://notebooks.sparksql/\" + credentials(\"filename\"))\n",
    "data_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a Spark RDD and you can begin analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dashdb\"></a>\n",
    "## Load data from dashDB\n",
    "\n",
    "dashDB is a data warehousing and analytics solution. Use dashDB to store relational data, including special data types such as geospatial data. You can leverage the in-memory database technology to use both columnar and row-based tables. \n",
    "\n",
    "You must have an IBM dashdDB for Bluemix service instance. In the notebook, select **Palette>Data Sources**. Click **Add Source**, select **From Bluemix**, and choose your dashDB instance. The dashDB instance name appears in the **Data Source** pane. \n",
    "\n",
    "Click the next code cell and use the `Insert to code` function below the dashDB instance name in the **Data Source** pane to add the dashDB credentials. \n",
    "\n",
    "When you select the `Insert to code` function, a code cell with a Scala hashmap is created for you. Adjust the credentials in the dictionary to correspond with the credentials inserted by the `Insert to code function` and run the dictionary code cell. The access credentials to the dashDB  instance in the dictionary are provided for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the credentials of your dashDB instance that contains your data, run the next cell to load this data. Be sure to set the `TABLENAME` variable to the name of the table in your DashDB you would like to access.\n",
    "\n",
    "The code in the cell reads the credentials and loads the data from dashBD into a DataFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.util.Properties\n",
    "import collection.JavaConversions._\n",
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlctx = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val TABLENAME = \"<name>\"\n",
    "\n",
    "val propMap = mapAsJavaMap(Map(\"user\"->credentials(\"username\"), \"password\"->credentials(\"password\")))\n",
    "val table = credentials(\"username\") + \".\" + tablename\n",
    "\n",
    "val props = new Properties()\n",
    "props.putAll(propMap)\n",
    "\n",
    "val df = sqlctx.read.jdbc(credentials(\"jdbcurl\"), table, properties=props)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a Spark DataFrame and you can begin analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cloudant\"></a>\n",
    "## Load data from a Cloudant database\n",
    "Cloudant is a NoSQL database as a service (DBaaS) built to scale globally, run nonstop, and handle a wide variety of data types like JSON, full-text, and geospatial. Cloudant NoSQL DB is an operational data store optimized to handle concurrent reads and  writes and to provide high availability and data durability.\n",
    "\n",
    "You must have an IBM Cloudant NoSQL Database for Bluemix service instance. In the notebook, select **Palette>Data Sources**. Click **Add Source**, select **From Bluemix**, and choose your Cloudant NoSQL DB instance. The Cloudant NoSQL DB instance name appears in the **Data Source** pane. \n",
    "\n",
    "Click the next code cell and use the `Insert to code` function below the Cloudant NoSQL DB instance name in the **Data Source** pane to add the Cloudant NoSQL DB instance credentials. \n",
    "\n",
    "Adjust the credentials in the Scala hashmap, which is prepared for you, to correspond with the credentials inserted by the `Insert to code` function and run the dictionary code cell. The access credentials to your Cloudant NoSQL DB instance in the dictionary are provided for convenience for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the credentials of your Cloudant instance that contains your data, run the next cell to load this data. Be sure to set the `DBNAME` variable to the name of the database in your Cloudant service you would like to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlctx = new SQLContext(sc)\n",
    "\n",
    "val DBNAME = \"<name>\"\n",
    "\n",
    "val df = sqlctx.read.format(\"com.cloudant.spark\").\n",
    "option(\"cloudant.host\", credentials(\"host\")).\n",
    "option(\"cloudant.username\", credentials(\"username\")).\n",
    "option(\"cloudant.password\", credentials(\"password\")).\n",
    "load(DBNAME)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a Spark DataFrame and you can begin analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"postgresql\"></a>\n",
    "## Load data from a PostgreSQL database\n",
    "PostgreSQL is an object-relational database system offered as a Bluemix service. It must be paired with an existing [Compose](https://www.compose.com/) account to be used.\n",
    "\n",
    "First we will load the jar for the necessary jdbc driver, then set the credentials of the table we would like to access. These can be found in your Compose account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%Addjar https://jdbc.postgresql.org/download/postgresql-9.4.1208.jre6.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val host = \"<host>\"\n",
    "val port = \"<port>\"\n",
    "val user = \"<user>\"\n",
    "val password = \"<password>\"\n",
    "val dbname = \"<db>\"\n",
    "val dbtable = \"<table>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this cell to have the jdbc driver load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlctx = new SQLContext(sc)\n",
    "\n",
    "val df = sqlctx.read.format(\"jdbc\").\n",
    "                    option(\"url\", \"jdbc:postgresql://\"+host+\":\"+port+\"/\"+dbname+\"?user=\"+user+\"&password=\"+password).\n",
    "                    option(\"dbtable\", dbtable).\n",
    "                    option(\"driver\", \"org.postgresql.Driver\").\n",
    "                    load()\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.sql.DataFrame` and you can start analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to load data from an Object Storage V3, dashDB, or Cloudant instance to a notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}