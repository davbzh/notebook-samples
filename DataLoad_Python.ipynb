{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load data into a notebook from different sources (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can start analyzing your data, you have to load the data from a data source. You can store your data in many different data sources. This reference notebook shows you how to load and integrate data in a notebook from the following data sources:\n",
    "-  Object Storage V3\n",
    "-  dashDB\n",
    "-  Cloudant\n",
    "-  MongoDB\n",
    "-  PostgreSQL\n",
    "\n",
    "The notebook sample code shows you how to load data into a notebook by using Python and the PySpark stack. You can copy and paste these code snippets into the notebook you are developing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Getpass](#getpass)\n",
    "- [Load data from Object Storage V3](#osv3)\n",
    "  - [Load data by using Python](#osv3_python)\n",
    "  - [Load data by using PySpark](#osv3_pyspark)\n",
    "  - [Load data by using Stocator](#osv3_stocator)\n",
    "- [Load data from dashDB](#dashdb)\n",
    "  - [Load data by using PySpark](#dashdb_pyspark)\n",
    "- [Load data from a Cloudant database](#cloudant)\n",
    "  - [Load data by using Python](#cloudant_python)\n",
    "  - [Load data by using PySpark](#cloudant_pyspark)\n",
    "- [Load data from MongoDB](#mongodb)\n",
    "- [Load data from a PostgreSQL database](#postgresql)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"getpass\"></a>\n",
    "## Getpass\n",
    "The `getpass()` function can be used to enter a password into your credentials. This is useful for sharing notebooks or presenting when you do not want to publicly display your password. First, we import `getpass`, then we replace the value of our `credentials` dictionary's `password` key with `getpass.getpass()`. Now instead of the cell displaying your password, you will be prompted to enter it upon execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "#insert this into password's value in dictionary\n",
    "password = getpass.getpass()\n",
    "#display that it works\n",
    "print password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3\"></a>\n",
    "## Load data from Object Storage V3\n",
    "IBM® Object Storage for Bluemix® provides you with access to a fully provisioned Swift Object Storage account to manage your data. Object Storage uses OpenStack Identity (Keystone) for authentication and can be accessed directly by using [OpenStack Object Storage (Swift) API v3](http://developer.openstack.org/api-ref-identity-v3.html#credentials-v3). \n",
    "\n",
    "When you load data for use in a notebook, the data file is stored in the Object Storage instance associated with your Spark service.\n",
    "\n",
    "Click the next code cell to set the focus on the cell. Now add the credentials to access the data file to this code cell by selecting **Palette>Data Sources** and clicking the `Insert to code` function below the data file in the **Data Source** pane.\n",
    "\n",
    "When you select the `Insert to code` function, a code cell with a Python dictionary is created for you. Adjust the credentials in the dictionary to correspond with the credentials inserted by the `Insert to code function` and run the dictionary code cell. The access credentials to the Object Storage instance in the dictionary are provided for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3_python\"></a>\n",
    "### Load data by using Python\n",
    "\n",
    "Run the next cells to load the data from a file in Object Storage by using Python functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, StringIO, json\n",
    "\n",
    "def get_file_content(credentials):\n",
    "    \"\"\"For given credentials, this functions returns a StringIO object containg the file content \n",
    "    from the associated Bluemix Object Storage V3.\"\"\"\n",
    "\n",
    "    url1 = ''.join([credentials['auth_url'], '/v3/auth/tokens'])\n",
    "    data = {'auth': {'identity': {'methods': ['password'],\n",
    "                                  'password': {'user': {'name': credentials['username'],\n",
    "                                                        'domain': {'id': credentials['domain_id']},\n",
    "                                                        'password': credentials['password']}}}}}\n",
    "    headers1 = {'Content-Type': 'application/json'}\n",
    "    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n",
    "    resp1_body = resp1.json()\n",
    "    for e1 in resp1_body['token']['catalog']:\n",
    "        if(e1['type']=='object-store'):\n",
    "            for e2 in e1['endpoints']:\n",
    "                if(e2['interface']=='public'and e2['region']==credentials['region']):\n",
    "                    url2 = ''.join([e2['url'],'/', credentials['container'], '/', credentials['filename']])\n",
    "                    print url2\n",
    "\n",
    "    s_subject_token = resp1.headers['x-subject-token']\n",
    "    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n",
    "    resp2 = requests.get(url=url2, headers=headers2)\n",
    "    return StringIO.StringIO(resp2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(get_file_content(credentials))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pandas.DataFrame` and you can begin analyzing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3_pyspark\"></a>\n",
    "### Load data by using PySpark\n",
    "\n",
    "Before you can access data in the data file in Object Storage by using the [`SparkContext`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.html#pyspark.SparkContext) object, you must set the Hadoop configuration by using the following configuration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(credentials):\n",
    "    \"\"\"This function sets the Hadoop configuration with given credentials, \n",
    "    so it is possible to access data using SparkContext\"\"\"\n",
    "    \n",
    "    prefix = \"fs.swift.service.\" + credentials['name']\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + \".auth.url\", credentials['auth_url']+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials['project_id'])\n",
    "    hconf.set(prefix + \".username\", credentials['user_id'])\n",
    "    hconf.set(prefix + \".password\", credentials['password'])\n",
    "    hconf.setInt(prefix + \".http.port\", 8080)\n",
    "    hconf.set(prefix + \".region\", credentials['region'])\n",
    "    hconf.setBoolean(prefix + \".public\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Hadoop configuration and give it a name, for example, `keystone`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# you can choose any alphanumeric name\n",
    "credentials['name'] = 'keystone'\n",
    "set_hadoop_config(credentials)\n",
    "\n",
    "data_rdd = sc.textFile(\"swift://\" + credentials['container'] + \".\" + credentials['name'] + \"/\" + credentials['filename'])\n",
    "data_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.RDD` and you can begin analyzing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"osv3_stocator\"></a>\n",
    "### Load data using Stocator\n",
    "Stocator is a storage connector for Spark that eliminates some of the unnecessary Hadoop drivers that are not needed to interact with object storage. Stocator's Hadoop configuration can be set by using the following configuration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(name, credentials):\n",
    "    prefix = \"fs.swift2d.service.\" + name \n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(\"fs.swift2d.impl\", \"com.ibm.stocator.fs.ObjectStoreFileSystem\")\n",
    "    hconf.set(prefix + \".auth.url\", credentials['auth_url']+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials['project_id'])\n",
    "    hconf.set(prefix + \".username\", credentials['user_id'])\n",
    "    hconf.set(prefix + \".password\", credentials['password'])\n",
    "    hconf.setInt(prefix + \".http.port\", 8080)\n",
    "    hconf.set(prefix + \".region\", credentials['region'])\n",
    "    hconf.setBoolean(prefix + \".public\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Hadoop configuration and check to see that it is set to stocator. The output of this cell should read `com.ibm.stocator.fs.ObjectStoreFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "set_hadoop_config(\"sparksql\", credentials)\n",
    "print sc._jsc.hadoopConfiguration().get(\"fs.swift2d.impl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILENAME = credentials[\"filename\"]\n",
    "FILENAME2D = \"swift2d://notebooks.sparksql/\" + FILENAME\n",
    "\n",
    "data_rdd = sc.textFile(FILENAME2D)\n",
    "data_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.RDD` and you can begin analyzing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dashdb\"></a>\n",
    "## Load data from dashDB\n",
    "\n",
    "dashDB is a data warehousing and analytics solution. Use dashDB to store relational data, including special data types such as geospatial data. You can leverage the in-memory database technology to use both columnar and row-based tables. \n",
    "\n",
    "You must have an IBM dashdDB for Bluemix service instance. In the notebook, select **Palette>Data Sources**. Click **Add Source**, select **From Bluemix**, and choose your dashDB instance. The dashDB instance name appears in the **Data Source** pane. \n",
    "\n",
    "Click the next code cell and use the `Insert to code` function below the dashDB instance name in the **Data Source** pane to add the dashDB credentials. \n",
    "\n",
    "When you select the `Insert to code` function, a code cell with a Python dictionary is created for you. Adjust the credentials in the dictionary to correspond with the credentials inserted by the `Insert to code function` and run the dictionary code cell. The access credentials to the dashDB  instance in the dictionary are provided for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dashdb_pyspark\"></a>\n",
    "### Load data by using PySpark\n",
    "\n",
    "Add the credentials of your dashDB instance that contains your data and run the next cell to load this data. Be sure to set the `TABLENAME` variable to the name of the table in your DashDB you would like to access.\n",
    "\n",
    "The code in the cell reads the credentials and loads the data from dashBD into a DataFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "TABLENAME = \"<name>\"\n",
    "\n",
    "props = {}\n",
    "props['user'] = credentials_2['username']\n",
    "props['password'] = credentials_2['password']\n",
    "\n",
    "table = credentials_2['username'] + \".\" + TABLENAME\n",
    "\n",
    "data_df = sqlContext.read.jdbc(credentials_2['jdbcurl'],table,properties=props)\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.sql.DataFrame` and you can analyze it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cloudant\"></a>\n",
    "## Load data from a Cloudant database\n",
    "Cloudant is a NoSQL database as a service (DBaaS) built to scale globally, run nonstop, and handle a wide variety of data types like JSON, full-text, and geospatial. Cloudant NoSQL DB is an operational data store optimized to handle concurrent reads and  writes and to provide high availability and data durability.\n",
    "\n",
    "You must have an IBM Cloudant NoSQL Database for Bluemix service instance. In the notebook, select **Palette>Data Sources**. Click **Add Source**, select **From Bluemix**, and choose your Cloudant NoSQL DB instance. The Cloudant NoSQL DB instance name appears in the **Data Source** pane. \n",
    "\n",
    "Click the next code cell and use the `Insert to code` function below the Cloudant NoSQL DB instance name in the **Data Source** pane to add the Cloudant NoSQL DB instance credentials. \n",
    "\n",
    "Adjust the credentials in the Python dictionary, which is prepared for you, to correspond with the credentials inserted by the `Insert to code` function and run the dictionary code cell. The access credentials to your Cloudant NoSQL DB instance in the dictionary are provided for convenience for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cloudant_python\"></a>\n",
    "### Load data by using Python\n",
    "\n",
    "Before you begin loading data from a Cloudant NoSQL DB instance to your notebook, ensure that you are using the latest database version. Do not use Cloudant 0.5.10 or earlier. For more information see [the Python library for Cloudant and CouchDB](https://github.com/cloudant/python-cloudant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `cloudant` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user cloudant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cloudant.client import Cloudant\n",
    "from cloudant.result import Result\n",
    "import pandas as pd, json\n",
    "\n",
    "client = Cloudant(credentials_3['username'], credentials_3['password'], url=credentials_3['url'])\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all existing databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client.all_dbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in database name \n",
    "db_name = 'test_db'\n",
    "my_database = client[db_name]\n",
    "result_collection = Result(my_database.all_docs, include_docs=True)\n",
    "data_df = pd.DataFrame([item['doc'] for item in result_collection])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pandas.DataFrame` and you can start analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cloudant_pyspark\"></a>\n",
    "### Load data by using PySpark\n",
    "\n",
    "Add the credentials of your Cloudant NoSQL DB instance that contains your data and run the next cell to load this data.\n",
    "\n",
    "The code in the cell reads the credentials and loads the data from Cloudant into a DataFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# fill in database name \n",
    "db_name = \"test_db\"\n",
    "data_df = sqlContext.read.format(\"com.cloudant.spark\")\\\n",
    ".option(\"cloudant.host\",credentials_3['host'])\\\n",
    ".option(\"cloudant.username\",credentials_3['username'])\\\n",
    ".option(\"cloudant.password\",credentials_3['password'])\\\n",
    ".load(db_name)\n",
    "\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data in a `pyspark.sql.DataFrame` and you can start analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mongodb\"></a>\n",
    "## Load data from MongoDB\n",
    "MongoDB is an NoSQL, document-oriented database. MongoDB favors JSON-like documents over the traditional relational table database structure.\n",
    "\n",
    "MongoDB is offered as a Bluemix service which must be paired with an existing [Compose](https://www.compose.com/) account.\n",
    "\n",
    "First, we will install and import the necessary drivers and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user pymongo\n",
    "\n",
    "import ssl\n",
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following connection credentials can be found on your Compose account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host = '<host>'\n",
    "database = '<db>'\n",
    "collection = '<col>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient(host+database, ssl=True, ssl_cert_reqs=ssl.CERT_NONE)\n",
    "db = client[database]\n",
    "col = db[collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read the credentials and load the data from the specified collection into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = col.find()\n",
    "json_content = []\n",
    "json_file = 'mongo.json'\n",
    "\n",
    "for doc in cursor:\n",
    "    doc['_id'] = str(doc['_id'])\n",
    "    json_content.append(doc)\n",
    "data = json.dumps(json_content)\n",
    "\n",
    "with open(json_file, 'w') as text_file:\n",
    "      text_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.json(json_file)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is in the DataFrame, we can clean up the json file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !rm <json_file>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.sql.DataFrame` and you can start analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"postgresql\"></a>\n",
    "## Load data from a PostgreSQL database\n",
    "PostgreSQL is an object-relational database system offered as a Bluemix service. It must be paired with an existing [Compose](https://www.compose.com/) account to be used.\n",
    "\n",
    "The following jar must be added in order for the PostgreSQL jdbc driver to work. However, Python notebooks do not support the `%Addjar` magic function. To load the jar, run the following cell in a Scala notebook running on the same Spark instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%Addjar https://jdbc.postgresql.org/download/postgresql-9.4.1208.jre6.jar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will set the credentials of the table we would like to access. These can be found in your Compose account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host = '<host>'\n",
    "port = '<port>'\n",
    "user = '<user>'\n",
    "password = '<password>'\n",
    "dbname = '<db>'\n",
    "dbtable = '<table>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this cell to have the jdbc driver load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.format('jdbc')\\\n",
    "                    .options(url='jdbc:postgresql://'+host+':'+port+'/'+dbname+'?user='+user+'&password='+password, dbtable=dbtable)\\\n",
    "                    .load()\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your data is in a `pyspark.sql.DataFrame` and you can start analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to load data from an Object Storage V3, dashDB, Cloudant, MongoDB or PostgreSQL instance to a notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}